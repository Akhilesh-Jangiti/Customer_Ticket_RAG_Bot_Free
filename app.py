# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18xxwqUSTfCGLOtWUVVAiGXvdw3TDlzUP
"""

!pip install -U langchain langchain-community
!pip install faiss-cpu sentence-transformers transformers streamlit

import streamlit as st
import pandas as pd
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# Load your customer support tickets
from google.colab import files
uploaded = files.upload()

# Load the dataset
@st.cache_data
def load_data():
    df = pd.read_csv("customer_support_tickets.csv")
    documents = [
        Document(
            page_content=f"Ticket Subject: {row['Ticket Subject']}\nTicket Description: {row['Ticket Description']}",
            metadata={"ticket_id": row["Ticket ID"]}
        )
        for _, row in df.iterrows()
    ]

# Load the dataset and FAQs into LangChain Documents
@st.cache_data
def load_data():
    # Sample FAQs
    faqs = [
        {"Ticket ID": "FAQ001", "Ticket Subject": "Cancel Order", "Ticket Description": "Go to My Orders â†’ Cancel."},
        {"Ticket ID": "FAQ002", "Ticket Subject": "Refund Policy", "Ticket Description": "Refund available within 14 days if item is in original condition."},
    ]

    documents = []  # âœ… Define the list first

    for faq in faqs:
        documents.append(Document(
            page_content=f"Ticket Subject: {faq['Ticket Subject']}\nTicket Description: {faq['Ticket Description']}",
            metadata={"ticket_id": faq["Ticket ID"]}
        ))

    return documents  # âœ… NOW it's inside the function

# Initialize components
@st.cache_resource
def setup_rag(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(documents)

    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(chunks, embedding)

    model_name = "google/flan-t5-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_new_tokens=256)
    llm = HuggingFacePipeline(pipeline=pipe)

    rag_chain = RetrievalQA.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_type="similarity", k=3),
        return_source_documents=True
    )
    return rag_chain

# Streamlit UI
st.title("ðŸ¤– Customer Support RAG Chatbot (GPT-Free)")
st.write("Ask me a question related to customer orders, refunds, or delivery.")

query = st.text_input("Your Question:", "")

if query:
    with st.spinner("Thinking..."):
        docs = load_data()
        chain = setup_rag(docs)
        result = chain({"query": query})
        st.markdown("### ðŸ’¬ Answer")
        st.write(result["result"])
        st.markdown("### ðŸ“š Sources")
        for doc in result["source_documents"]:
            st.write(f"â€¢ {doc.metadata['ticket_id']}")